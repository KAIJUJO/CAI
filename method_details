CAI-System V2.1 技术规格说明书 (Revised)
项目代号: CAI-System (Causal Active Inference)版本: V2.1-Amortized (Grounded Distillation Edition)核心理念: 脑眼分离 (Decoupled Eye-Brain) + 摊销因果感知 (Amortized Causal Perception) + 程序化逻辑验证 (Programmatic Verification)
1. 系统总体架构 (System Architecture)
系统由四个串行耦合的流水线组成，旨在实现“看着图（感知）、写代码（推理）、自我纠错（训练）、主动探索（执行）”的闭环：
感知层 (V1 - Leaky AS-IB)：基于 摊销空间信息瓶颈 的视觉适配器，利用 Grounded-SAM 蒸馏实现零梯度开销的因果特征提取。
推理层 (V2 - LogicPro)：基于 PIPS  路由的神经符号引擎，动态选择 Python 代码执行或文本推理。
训练层 (V3 - PURE)：基于 Min-Form 1 信用分配的 GRPO 策略优化器，消除长链推理的幻觉。
执行层 (V4 - Active Loop)：基于 EFE (期望自由能) 的主动感知规划，控制 Zoom/Crop 动作。
2. 模块一：感知层 (Amortized Causal Perception)
目标：在推理阶段，通过单次前向传播实现背景噪声的解耦。针对 V2.0 存在的“特征崩塌”风险，引入防失明机制；针对 IV-VQA 标注缺失问题，引入混合监督蒸馏。
2.1 核心算法：Leaky AS-IB (防失明摊销 S-IB)
我们不再在推理时计算梯度，而是训练一个 Learnable S-IB Adapter 将因果显著性“内化”。针对冷启动问题，显式引入软门控残差。
架构设计：
Backbone (冻结)：使用 SigLIP-SO400M 2，负责提取原始高维视觉特征 $Z_{raw}$。
Adapter (训练)：轻量级 Mask Generator $f_\theta$，输入 $Z_{raw}$，输出因果掩码 $M_{pred}$。
前向传播逻辑 (Leaky Bottleneck)：$$M_{pred} = f_\theta(Z_{raw}, Q_{emb})$$$$Z_{causal} = Z_{raw} \odot M_{pred} + \mathbf{\alpha \cdot Z_{raw}} + \epsilon \cdot (1 - M_{pred})$$
参数释义：
$M_{pred} \in 3$：预测的前景掩码。
$\alpha$ (Leaky Coefficient)：可学习的残差系数（初始化为 0.1）。即使 $M_{pred}$ 在训练初期全为 0，$\alpha$ 项保证 LLM 仍能获取基础视觉信息，防止“失明”导致的梯度截断。
$\epsilon$：高斯噪声，用于切断背景信息流。
2.2 训练策略：混合监督蒸馏 (Hybrid Distillation)
废弃不稳定的一致性损失，采用 分级监督 + 非对称增强 策略 3, 4。
混合数据分级 (Hierarchy)：
Tier 1 (Gold - 强监督)：对命中 GQA/COCO 的数据，直接使用 Ground Truth Masks 计算 Dice/MSE Loss。
Tier 2 (Silver - 伪标签)：对 IV-VQA 中未命中但存在物体的数据，利用 Grounded-SAM 生成伪标签 (Pseudo-Masks) 作为教师信号。
Tier 3 (Bronze - 鲁棒性)：对 IV-VQA 反事实负样本（如“图里没狗”），生成全零掩码，并结合 Focal Loss 解决正负样本不平衡问题。
非对称增强 (Asymmetric Augmentation)：
Student (Adapter) 输入模糊/低分辨率图像。
Teacher (Mask) 来自高清原图生成的 Grounded-SAM 结果。
目的：强迫 Adapter 学会在看不清细节时，依据模糊轮廓预测高价值区域，直接为后续的 Active Zoom 机制铺路。
3. 模块二：推理层 (Neuro-Symbolic Reasoning)
目标：解决 LLM “眼手不协调”的问题，通过代码执行确保推理过程的逻辑严密性。
3.1 核心流程：LogicPro Pipeline
基于 LogicPro 5 方法，我们将推理过程转化为程序生成任务。
Prompt 策略：
符号提取 (Perception Trace)：blocks = [{'color': 'red', 'x': 10}, ...]
逻辑构建 (Reasoning Trace)：if blocks.x > 5: ...
中间变量追踪 (Execution Trace)：print(f"step1_val={...}")
PIPS Router：部署轻量级分类器，基于问题复杂度动态决定是否调用代码引擎。
4. 模块三：策略优化 (PURE-GRPO RL)
目标：解决过程奖励模型 (PRM) 中的 Reward Hacking 问题，并优化显存占用。
4.1 核心算法：PURE (Min-Form Credit Assignment)
摒弃传统累加回报，采用 “短板效应” 评分来衡量推理质量。
价值函数 (Process Quality)：$$V(s_t) = \mathbb{E}_{\pi} \left\min(r_t, r_{t+1}, \dots, r_T) \right$$(只要推理链中存在任何一步逻辑断裂，整条路径价值归零)
4.2 策略更新：GRPO (Group Relative Policy Optimization)
采用 GRPO 替代 PPO，不再依赖 Critic 网络估计状态价值进行优势计算，而是基于组内采样的相对优劣。
优势计算 (Group Relative Advantage)：$$A_{i,t} = \frac{R_{i,t} - \text{mean}(\{R_{1..G}\})}{\text{std}(\{R_{1..G}\}) + \epsilon}$$
$G$：组内采样数量 (Group Size)。
$R_{i,t}$：第 $i$ 条采样路径在 $t$ 时刻的累积回报（包含 Min-Form 过程分 + 最终代码正确性得分）。
收益：大幅降低训练显存开销，更适合长链推理训练。
5. 模块四：执行层 (Active Perception Loop)
目标：实现“看不清就凑近看”的主动行为。
5.1 决策机制：EFE 近似
利用 PURE 训练好的 Value Head 作为不确定性度量。
EFE 代理公式：$$EFE(a) \approx - V_{PURE}(s_{current}) + \text{Cost}(a)$$
逻辑流：
观察：输入当前图像 $I$。
评估：$V_{pred} = \text{ValueHead}(I, Q)$。
决策：
若 $V_{pred} < \text{Threshold}$ (风险高/看不清) $\rightarrow$ 执行 Zoom/Crop。
若 $V_{pred} > \text{Threshold}$ (信心足) $\rightarrow$ 执行 Generate Code。
6. 实施路线修正 (Execution Roadmap)
Phase 1 (Data Synthesis): 使用 GPT-4o + LogicPro 流程，合成 110k 条代码推理数据。
风险控制：数据源包括 GQA, MathVista, CLEVR。注意： 生成数据时仅参考 PAC-Bench 的评估范式（如物理约束检查），严禁使用 PAC-Bench 原题，以避免数据泄露 6。
Phase 2 (Perception): 冻结 SigLIP，利用 Grounded-SAM 生成的伪标签和 GQA 真值，配合 Leaky Bottleneck 训练 S-IB Adapter。
Phase 3 (Cold Start): 使用 LogicPro 数据对 LLM + Adapter 进行 SFT，打通视觉符号到代码变量的接口。
Phase 4 (RL): 开启 PURE-GRPO 训练。重点监控 Min-Form Value Head 的收敛情况，确保它能准确预测“逻辑崩塌”的风险。
主要参考文献支持：
S-IB / VJP 原理: 3, 4 Spatial Information Bottleneck for Interpretable Visual Recognition
LogicPro: 5 LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning
PURE / Min-Form: 1 Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning
GRPO / PIPS:  DeepSeek-R1 / Once Upon an Input
